














#-----------------------------------Features-----------------------------------------------------------














	#Edges : [2773, 12551, 128239, 128239, 128239, 128239, 180126, 180126, 180126, 180126, 180126]
	#Layers : [20, 30, 30, 30, 30, 30, 20, 20, 20, 20, 20]
	
	
from keras.datasets import fashion_mnist
(train_X,train_Y), (test_X,test_Y) = fashion_mnist.load_data()

#Analyzing data, remove this section later
import numpy as np
from tensorflow.keras.utils import to_categorical     #Changed
import matplotlib.pyplot as plt

print('Training data shape : ', train_X.shape, train_Y.shape)
print('Testing data shape : ', test_X.shape, test_Y.shape)

# Find the unique numbers from the train labels
classes = np.unique(train_Y)
nClasses = len(classes)
#print('Total number of outputs : ', nClasses)
#print('Output classes : ', classes)


# Data pre processing $$WHY ARE WE DOING THIS
train_X = train_X.reshape(-1, 28,28, 1)
test_X = test_X.reshape(-1, 28,28, 1)
#train_X.shape, test_X.shape


#Converting data fromat from int8 to float32 AND rescaling pixel values to 0-1
train_X = train_X.astype('float32')
test_X = test_X.astype('float32')
train_X = train_X / 255.
test_X = test_X / 255.


#model The Data
import keras
from keras.models import Sequential,Input,Model
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
#from keras.layers.normalization import 
from keras.layers import LayerNormalization
from keras.layers.advanced_activations import LeakyReLU
from tensorflow import keras
import csv



digits_model = Sequential()
digits_model.add(Conv2D(4, kernel_size=(3, 3),activation='linear',padding='same',input_shape=(28,28,1)))
digits_model.add(LeakyReLU(alpha=0.1))
digits_model.add(MaxPooling2D((2, 2),padding='same'))
digits_model.add(Dropout(0.25))
digits_model.add(Conv2D(8, (3, 3), activation='linear',padding='same'))
digits_model.add(LeakyReLU(alpha=0.1))
digits_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))
digits_model.add(Dropout(0.25))
digits_model.add(Conv2D(16, (3, 3), activation='linear',padding='same'))
digits_model.add(LeakyReLU(alpha=0.1))                  
digits_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))
digits_model.add(Dropout(0.4))
digits_model.add(Conv2D(32, (3, 3), activation='linear',padding='same'))
digits_model.add(LeakyReLU(alpha=0.1))                  
digits_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))
digits_model.add(Dropout(0.4))
digits_model.add(Flatten())

digits_model.summary()



train_data = list()

for i, img in enumerate(train_X):
	if (i+1)%100==0:
		print(i+1)
	img = np.expand_dims(img, axis=0)
	fv = digits_model.predict(img)[0]
	instance = [i+1, train_Y[i]] + list(fv)
	train_data.append(instance)


with open('ftrain.tsv', 'w') as file:
	wr = csv.writer(file,delimiter='\t')
	for row in train_data:
			wr.writerow(row)


test_data = list()

for i, img in enumerate(test_X):
	if (i+1)%100==0:
		print(i+1)
	img = np.expand_dims(img, axis=0)
	fv = digits_model.predict(img)[0]
	instance = [i+1, test_Y[i]] + list(fv)
	test_data.append(instance)


with open('ftest.tsv', 'w') as file:
	wr = csv.writer(file,delimiter='\t')
	for row in test_data:
			wr.writerow(row)














































#--------------------------------------------------SPN Model------------------------------------------------

























'''

This Code is used to learn and evaluate
the SPN models for the given datasets
using the LearnSPN algorithm


'''

import numpy as np

from spn.algorithms.StructureLearning import get_next_operation, learn_structure
from spn.algorithms.CnetStructureLearning import get_next_operation_cnet, learn_structure_cnet
from spn.algorithms.Validity import is_valid
from spn.algorithms.Statistics import get_structure_stats_dict

from spn.structure.Base import Sum, assign_ids

from spn.structure.leaves.histogram.Histograms import create_histogram_leaf
from spn.structure.leaves.parametric.Parametric import create_parametric_leaf
from spn.structure.leaves.piecewise.PiecewiseLinear import create_piecewise_leaf
from spn.structure.leaves.cltree.CLTree import create_cltree_leaf
from spn.algorithms.splitting.Conditioning import (
	get_split_rows_naive_mle_conditioning,
	get_split_rows_random_conditioning,
)
from spn.structure.leaves.parametric.Parametric import Categorical, Gaussian

from spn.algorithms.LearningWrappers import learn_parametric, learn_classifier


import warnings
warnings.filterwarnings('ignore')



import pandas as pd
from spn.structure.Base import Context
from spn.structure.StatisticalTypes import MetaType
from spn.algorithms.Statistics import get_structure_stats_dict
import matplotlib.pyplot as plt
from os import path as pth
import sys, os
import time
import multiprocessing
import pickle
import random
from keras.datasets import mnist
from PIL import Image

#Initialize parameters

cols="rdc"
rows="kmeans"
min_instances_slice=200
threshold=0.3
ohe=False
leaves = create_histogram_leaf
rand_gen=None
cpus=-1


path = "mnist"

dataset = "mnist"



(x_train, y_train), (x_test, y_test) = mnist.load_data()


#Create output directory 
print(f"\n\n\n{dataset}\n\n\n")
if not pth.exists(f'{path}/models'):
	try:
		os.makedirs(f'{path}/models')
	except OSError:
		print ("Creation of the directory %s failed" % path)
		sys.exit()

		

train = list()
for i, x in enumerate(x_train):
	x = [y_train[i]] + list(np.reshape(x, (x.shape[0]*x.shape[1])))
	train.append(x)

train = np.array(train)


ds_context = Context(parametric_types=[Categorical]+ [Gaussian]*(train.shape[1]-1))
ds_context.add_domains(train)

print("\n\nLearning SPN")
spn = learn_classifier(train, ds_context, learn_parametric, 0)
print("\nSPN Learned!")


file = open(f"{path}/models/spn_{dataset}.pkle",'wb')
pickle.dump(spn, file)
file.close()


test = list()
for i, x in enumerate(x_test):
	x = [np.nan] + list(np.reshape(x, (x.shape[0]*x.shape[1])))
	test.append(x)

test = np.array(test)




nodes = get_structure_stats_dict(spn)["nodes"]
parameters = get_structure_stats_dict(spn)["parameters"]
layers = get_structure_stats_dict(spn)["layers"]


print(f"\n\tNodes : {nodes}")
print(f"\n\tParameters : {parameters}")
print(f"\n\tLayers : {layers}")

from spn.algorithms.MPE import mpe


results = mpe(spn, test)

pred = list(results[:,0])
true = list(y_test)


from sklearn import metrics

report = metrics.classification_report(true, pred)
print(f'\n\nReport : \n{report}')

prfs = metrics.precision_recall_fscore_support(true, pred)
prfs_micro = metrics.precision_recall_fscore_support(true, pred, average='micro')
cm = metrics.confusion_matrix(true, pred)

print(f"\n\t{prfs}")
print(f"\n\t{prfs_micro}")
print(f"\n\tConfusion Matrix : {cm}")


f = open(f"{path}/{dataset}/stats14.txt", "w")
f.write(f"\n{dataset}")
f.write(f"\n\tRun Time : {runtime}")
f.write(f"\n\tNodes : {nodes}")
f.write(f"\n\tParameters : {parameters}")
f.write(f"\n\tLayers : {layers}")
f.write(f"\n\tReport : \n{report}")
f.write(f"\n\tConfusion Matrix : {cm}")
f.write(f"\n\t{prfs}")
f.write(f"\n\t{prfs_micro}")
f.close()